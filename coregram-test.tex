% -*- coding: utf-8 -*- 
\documentclass[number=45,series=eotms]{langsci}                          
                               

\title{Danish in \newlineCover Head-Driven \newlineCover\newlineSpine Phrase Structure \newlineCover Grammar  }                        
\author{Stefan M\"uller, \newlineCover Pollet Samvelian, \newlineCover Olivier Bonami}
%\BackTitle{title text of the back page}
%\BackBody{body text of the back page}                                            
 
\begin{document}              
       
                
      
\maketitle                

\tableofcontents
  

%\setmainfont{Code2000} 
\setmainfont[Ligatures=TeX]{Linux Libertine O}

\usepackage{maltese}


\usepackage{tikz-qtree}
\usetikzlibrary{positioning}

\usepackage{fontspec}\newfontfamily\Parsifont[Script=Arabic]{XB Niloofar}
\usepackage{bidi}
\newcommand{\PRL}[1]{\RL{\Parsifont #1}}


\begin{document}
%http://tex.stackexchange.com/questions/64830/using-tipa-with-fontspec
%\renewcommand\textipa[1]{{\fontfamily{cmr}\tipaencoding #1}}
\maketitle

%\centerline{To appear in Language}


\renewcommand{\il}[1]{}
\submitOrNormal{
\setcounter{endnote}{0}
\renewcommand{\theendnote}{\fnsymbol{endnote}}
}{
\setcounter{footnote}{1}
\renewcommand{\thefootnote}{\fnsymbol{footnote}}
}

\begin{abstract}
This paper describes the CoreGram project, a multilingual grammar engineering project that develops
HPSG grammars for several typologically diverse languages that share a common core. The paper
provides a general motivation for doing theoretical linguistics the way it is done in the CoreGram
project and therefore is not targeted to computational linguists exclusively. I argue for a
constraint-based approach to language rather than a generative-enumerative one and discuss issues of
formalization. Recent advantages in the language acquisition research are mentioned and conclusions
on how theories should be constructed are drawn. The paper discusses some of the highlights in the
implemented grammars, gives a brief overview of central theoretical concepts and their
implementation in TRALE and compares the CoreGram project with other multilingual grammar engineering projects.
\end{abstract}


\footnotetext{%
I thank
% Ivan
% Frank Richter
% Gerald
% FU-Profs, Eisenberg, Wunderlich
% Bob Levine
% Pullum
% Gibson
% Federenko
% Tibor
for comments on earlier versions of this paper.

Parts of this paper were presented at the MIT, at the Gibson Lab, Brain and Cognitive Sciences. I
thank Ted Gibson for the invitations and all the participants for discussion.

The work reported in this paper was supported by grants by the Deutsche Forschungsgemeinschaft
(InfStruk MU~2822/1-1, SFB 632 A6, DanGram MU~2822/2-1, PerGram MU 2822/3-1, and ChinGram MU 2822/5-1).
%
}

\submitOrNormal{
\renewcommand{\theendnote}{\arabic{endnote}}
\setcounter{endnote}{0}
}{
\renewcommand{\thefootnote}{\arabic{footnote}}
\setcounter{footnote}{0}}

\noindent
Keywords: Universal Grammar, Head-Driven Phrase Structure Grammar, Multilingual Grammar Engineering 

\section{Overview and Motivation}

The goal of the CoreGram project is the development of large
scale computer processable grammar fragments of several languages that share a common
core. The theoretical framework is Head-Driven Phrase Structure Grammar (HPSG,
\citew{ps2,MuellerLehrbuch1}). Currently we work on the following languages: 
\begin{itemize}
\item German\il{German}  \citep{MuellerLehrbuch1,MuellerPredication,MuellerCopula,MOe2011a,MOe2013a,MuellerGS}
\item Danish  \citep{Oersnes2009a,MuellerPredication,MuellerCopula,MOe2011a,MOe2013a,MOe2013b,MOeDanish}
\item Persian\il{Persian} \citep*{MuellerPersian,MG2010a,PersianBook}
\item Maltese\il{Maltese} \citep{MuellerMalteseSketch}
\item Mandarin Chinese\il{Mandarin Chinese} \citep{Lipenkova2009a,ML2009a,MLChinese}
\item Yiddish\il{Yiddish} \citep{MOe2011a}
\item English\il{English} \citep{MuellerPredication,MuellerCopula,MOe2013a}
\item Hindi
\item Spanish\il{Spanish}
\item French\il{French}
\end{itemize}
\begin{sloppypar}
\noindent
For the implementation we use the TRALE system \citep*{MPR2002a-u,Penn2004a-u}, which allows for a
rather direct encoding of HPSG analyses \citep{MelnikHandWritten}. The grammars of German, Danish,
Persian, Maltese, and Mandarin Chinese are of non-trivial size and can be downloaded at
\url{http://hpsg.fu-berlin.de/Projects/CoreGram.html}. They are also part of the version of the Grammix
CD-rom \citep{Mueller2007b}. The grammars of Yiddish and English
are toy grammars that are used to verify cross-linguistic analyses of special phenomena. The grammar
of Hindi is also a small fragment. It was developed together with Shravan Vasishth during a seminar
at the University of Potsdam in 2006. The work
on Spanish and French is part of work in the Sonderforschungsbereich~632 which just started. See
\citew{Bildhauer2008a} for an implemented grammar of Spanish\il{Spanish} that will be converted into the format
of the grammars mentioned above. 
\end{sloppypar}

We believe that working out large scale computer"=implemented grammars is the best way to verify the
consistency of linguistic theories. Much linguistic work is published in journal articles but the
underlying assumptions of articles may be different so that it is difficult to imagine a coherent
view that incorporates all insights. Even for articles by the same author it is not guaranteed that
basic assumptions are shared between articles since it can take several years for individual papers
until they are published. Hence, I believe that books are the right format for describing linguistic
theories and ideally such theories are backed up by computer implementations. The larger fragments
of the CoreGram project will be documented in a series of book publications. The first book in this series was
\citew{MuellerLehrbuch1}, which describes a fragment of German that is implemented in the grammar
BerliGram\is{BerliGram}. Three further books are in preparation and will be submitted to the series
Implemented Grammars by Language Science Press: one on the Persian Grammar developed in the PerGram\is{PerGram}
project \citep*{PersianBook}, the Danish Grammar developed in the DanGram\is{DanGram}
project \citep*{MOeDanish} and the Mandarin Chinese grammar developed in the ChinGram project \citep{MLChinese}.


The paper will be structured as follows: the remainder of this section describes desiderata for
linguistic theories and discusses the importance of formalization with special focus on main-stream
theories and research programs like GB and Minimalism and Construction
Grammar. Section~\ref{sec-pos-motivation} discusses the way that theories of our linguistic
knowledge should be constructed. Section~\ref{sec-coverage-and-highlights} shows some highlights
from the various CoreGram grammars. Section~\ref{sec-basic-assumptions} discusses basic theoretical assumptions for the
treatment of valence, constituent order, morphology, semantics, and information structure and
Section~\ref{sec-implementation-details} provides details on how things are implemented in the TRALE
system. 
%Section~\ref{sec-coregram-setup} is a brief description of the technical setup of the CoreGram
%grammars. 
Section~\ref{sec-comparison} compares the
CoreGram project with other multilingual projects like ParGram and DELPH-IN, Section~\ref{sec-evaluation} deals with
evaluation and grammar profiling and testing and Section~\ref{sec-conclusions} draws some conclusions.

\subsection{Desiderata for Linguistic Theories}

This section discusses desiderata for linguistic theories and shows that the framework that is
assumed in the CoreGram project, namely HPSG, fullfills all of them.

\subsubsection{Non-transformational, Constraint-based Approach}

While first psycholinguistic experiments seemed to confirm the Derivational Theory of Complexity
\citep{MMK64a,SP65a,CO66a}, so that Chomsky assumed it to be correct until 1968 \citep[\page
  249--250]{Chomsky76b-u}, it was later shown that the initial experiments were flawed and that
transformations are not psycholinguistically real \citep*[\page 324]{FBG74a-u}. Since then it is
usually said that transformations are metaphors (for instance in
\citew[Footnote~4]{Chomsky2001a-u}). This of course begs the question why one should formulate ones
theories in metaphors (see also \citew[\page 599]{Jackendoff2011a}). This question is even more
pressing since a lot of Minimalist theorizing is done now under the label of Biolinguistics and the
assumed processes are claimed to be psycholinguistically real. For instance, Chomsky refers to
aspects of processing and memory requirements (Chomsky: \citeyear[\page 11, 12, 15]{Chomsky2001a-u};
\citeyear[\page 3, 12]{Chomsky2007a}; \citeyear[\page 138, 145, 146, 155]{Chomsky2008a}). See also
\citew[\page 440]{Marantz2005a} und \citew{Richards2010a}. However, structure building processes
that start with the combination of words and assume later reorderings are highly implausible from a
psycholinguistic point of view. As was pointed out by \citew{Labelle2007a} human short-term memory
simply is too limited to be able to compute complex structure in the way it is envisaged by current
Minimalist theories. Models that crucially rely on the order of the application of combination
operations fail since we neither use our linguistic knowledge exclusively bottom-up nor exclusively
top-down. \citet{Phillips2003a} suggested a theory variant that allows for incremental parsing, but
first, this is tailored towards parsing and ignores generation and second it is incompatible with
much of the rest of the Minimalist theories.

The way out of all of these problems is a clear separation between competence and performance and a
declarative constraint-based statement of linguistic constraints that do not make any claims about
the order of constraint application (\citealp{SW2007a}; \citealp[\page 600]{Jackendoff2011a}). The
order of application is constrained by performance models that are an important part of a theory
about language and that have to be combined with competence models. Proponents of usage-based
approaches often reject the competence/performance distinction, but as soon as a grammar contains
grammar rules or schemata that can be applied recursively (for instance a schema for relative
clauses, \citet*{BLT2009a} use a context-free grammar, a kind of grammar that clearly allows for
recursive structures) one has to explain why sentences have a maximal length, why we cannot do
center-embeddings with more than four levels and so on. These limitations are for instance due to
our short term memory and this has to be explained by a performance model that takes such factors
into account \citep{Gibson98a}.

HPSG is a constraint-based theory which does not make any claims on the order of application of
combinatorial processes. Theories in this framework are just statements about relations between
linguistic objects or between properties of linguistic objects and hence compatible with
psycholinguistic findings and processing models \citep{SW2007a}.

\citet{PS2001a,Pullum2007a} discuss further advantages of model theoretic and hence constraint-based
proposals: they allow the construction of partial structures, can explain graded grammaticality, and
no claims about the infinitude of language are necessary.

As Pullum and Scholz note we can assign the following structure to the fragment \emph{and of the} in
a sentence like (\mex{1}):
\ea
That cat is afraid of the dog and of the parrot.
\z
\begin{figure}[htb]
\centering
\begin{tikzpicture}
\tikzset{level 1+/.style={level distance=3\baselineskip}}
%\tikzset{frontier/.style={distance from root=23\baselineskip}}
\Tree[.PP
        PP
        [.{PP[{\sc coord} \emph{and} ]}
          [.Conj and ]
          [.{PP} 
            [.P of ]
            [.NP 
              [.Det the ]
              {\nbar}  ] ] ] 
]
\end{tikzpicture}
\caption{\label{fig-and-of-the}Structure of the fragment \emph{and of the} following
  \citew[\page 32]{PS2001a}}
\end{figure}
If we hear \emph{the} we know that an \nbar will follow, if we hear \emph{of}, we will know that
\emph{of} will head a PP. \emph{and} usually is part of symmetric coordinations, so we know that the
first part of the coordination will be a PP to. So, the information from lexical items and dominance
schemata licenses a complex structure in constraint-based models, while nothing is generated in
generative-enumerative models. \emph{and of the} is just not a member of a set with well-formed
expressions. 

This property of constraint-based approaches also comes in handy when we want to explain the
robustness of human sentence porcessing. In the case of unknown words information from syntax and
semantics can be used to draw inferences about the material that is missing. For instance in an
utterance like (\mex{1}) in which information about XXX is missing the hearer can infer that XXX
must be a verb and that it must have three arguments (This argument is also due to \citet{PS2001a},
who make it in a slightly different form).
\ea
Could you XXX me the salt, please?
\z
Hence, the hearer arrives at a prototypical ditransitive verb, a verb of change of possession, that
is, something like \emph{give} or \emph{pass}. In current Minimalist models nothing would happen
since structure building and movement is triggered by lexical items and their features, but if they
are absent/unknown the derivation does not start.\footnote{
\citet{Chomsky2007a} writes: \emph{A Merge-based system of derivation involves parallel
  operations. Thus if X and Y are merged, each has to be available, possibly constructed by
  (sometimes) iterated Merge. The process has a loose resemblance to early theories of generalized
  transformations, abandoned in the early 1960s for good reasons, now resurrected in a far simpler
  form for better reasons. But a generative system involves no temporal dimension. In this respect,
  generation of expressions is similar to other recursive processes such as construction of formal
  proofs. Intuitively, the proof ``begins'' with axioms and each line is added to earlier lines by
  rules of inference or additional axioms. But this implies no temporal ordering. It is simply a
  description of the structural properties of the geometrical object ``proof.'' The actual
  construction of a proof may well begin with its last line, involve independently generated lemmas,
  etc. The choice of axioms might come last. The same is true of generation vs production of an
  expression, a familiar competence-performance distinction.} This seems to indicate a
constraint-based position. However, even if a constraint-based view is assumed in principle, there
are lots of unsolved problems with specific proposal. For instance, some approaches assume the
existence of unvalued features that get a value during a derivation. What happens if the information
about these features is not availible to the hearer? According to Minimalist theorizing the
derivation should crash. But as (\mex{0}) just shows, it does not crash but results in partial,
underspecified information.
}

While constraint-based approaches can explain markedness of structures by the number and strength of
constraints that are violated by a given example, this is not possible in generative-enumerative
approaches. For a discussion of an attempt to get markedness into the picture of
generative-enumerative models see Chomsky, \citeyear[Chapter~5]{Chomsky75a};
\citeyear{Chomsky64a}. For a rejection of these proposals see \citew[\page 29]{PS2001a}.

Finally, constraint-based models do not have to make any claims about the infinitude of
language. While it is usually claimed that language is infinite by proponents of
generative-enumerative proposals no such claims are necessary in the model theoretic world. Those
who claim that language is infinite and biologically real are faced with the paradox that infinitely
many members of the infinite set are not biologically real, that is, they can never be realized
because of our limited resources \citep{Postal2009a}. Model theoretic approaches do not have to assume infinite sets
only to throw away most of them because of performance considerations, rather they pair the
performance model with the competence model directly and therefore get theories that are
psycholinguistically and biological plausible.

As I showed in Müller, \citeyear[Chapter~3.6.4]{MuellerGTBuch1}; \citeyear{MuellerUnifying}, the
combinatory operations of Minimalism as defined in \citew{Chomsky2008a} and \citew{Stabler2001a}
corresponds to three of the schemata used in HPSG grammars since at least \citew{ps2}: Merge
corresponds the Head-Specifier Schema and the Head-Complement Schema of HPSG and Move corresponds to
the Head-Filler Schema. So, if one leaves ideology and rhetorics away, one has a constraint-based
declarative formalization of Minimalist proposals. Of course a lot of questions have to be asked
about current Minimalist analyses, some of them being addressed in Section~\ref{sec-pos-motivation}.





\subsubsection{Sign-Based/Parallel Architecture}

As was shown by \citet{Marslen-Wilson75a} in the 70ies and confirmed later by many studies, we
process linguistic and non-linguistic information as soon as it is available to us and there is no
ordering of strictly encapsulated modules of processing. \citet{TSKES95a,TSKES96a} use eye-tracking
techniques to establish that we know the referents of NPs even if we just heard a determiner and an
adjective. The authors showed that stress on the adjective is interpreted immediately as a sign of
contrast and the right inferences are drawn and the only possible object in a specific scene is
looked at. The studies showed that constraints from phonology, syntax, and information structure are
evaluated immediately. Information is processed as soon as it is available. We can use it and draw
inferences even though we may not be able to assign a full syntactic structure to a certain phrase
yet. Such findings are compatible with architectures that assume that all linguistic levels are
accessible simultaneously as in HPSG and CxG and in Jackendoff's Parallel Architecture \citep{Jackendoff99a-u,Jackendoff2011a}.


\subsubsection{Possibility to Include Non-Headed and Phrasal Constructions}


I agree with \citet{Jackendoff2008a,Jackendoff2011a}, \citet{Jacobs2008a}, \citet{Sag2010b}, and
others that one needs more than some very general binary-branching schemata to deal with language in
its full richness in non-stipulative ways. Hence I believe that additional schemata or phrasal
constructions in the sense of CxG or Simpler Syntax \citep{CJ2005a} are needed. To what extent
phrasal constructions are needed and where Merge-like combinations together with a rich lexicon are
sufficient or rather necessary is an empirical issue and the CoreGram project contributes to this
discussion. See for instance \citew{ML2009a} for a phrasal treatment of serial verbs in Mandarin
Chinese and \citew{Oersnes2009a} for a phrasal treatment of preposed negation in Danish.


\subsubsection{Core/Periphery Distinction}

Given that the name of the project is CoreGram, some words on the Core/Periphery distinction are in
order.

\mbox{}\citet[\page 7--8]{Chomsky81a} suggests to divide languages into a core part and a periphery.
All regular parts belong to the core. The core grammar of a language is assumed to be an instance of
Universal Grammar (UG), the genetically determined innate language faculty of human beings. Idioms
and other irregular parts of a language belong to the periphery. Critics of Chomsky's Principle and
Parameters approach have pointed out that a rather large proportion of our languages consist of or
interact with irregular constructions and that the borders between core and periphery cannot been
drawn easily and often are motivated theory-internally only (\citealp[Kapitel~7]{Jackendoff97a}; \citealp{Culicover99a-u};
\citealp[\page 5]{GSag2000a-u}; \citealp[\page 48]{Newmeyer2005a}; \citealp[\page 619]{Kuhn2007a}).
%\LATER{\citep{Culicover99a}} 
% Newmeyer2005a:48 zitiert Chomsky mit Aussagen, dass die Unterscheidung abgeschafft werden sollte
For instance \citet*{NSW94a} pointed out that many English idioms interact with syntax. See also
\citew[\page 350]{MuellerGTBuch1} for interactions of verb placement, V2, and passive with idioms in German.

So, we do not think that it is justified to ignore phenomena that are claimed to belong to the
periphery. Rather we agree with \citet{BF99a} and \citet[\page 20--21]{Bender2008c} that studying
phenomena that are traditionally assigned to the periphery may discriminate between possible
analyses of the alleged core phenomena.

It should be noted however that the methodology described in Section~\ref{sec-data-driven-bu}
results in a separation of core and periphery: core constraints in the sense of the CoreGram project
are those constraints that are shared by at least two languages. This notion of periphery is very
different from the Chomskyan in that it only includes those phenomena in the periphery that are
unique for one language. Depending on the version of the theory we look at, the periphery in the
Chomskian sense includes phenomena like Exceptional Case Marking \citep[\page 70]{Chomsky81a} that are common in a lot of
languages and hence are assigned to the core in our setting. Please refer to
Section~\ref{sec-data-driven-bu} for details.

\subsection{Formalization, Computer Implementations, and Theory Verification}


The work of Noam Chomsky pioneered the formalization of linguistic theories. In his early writings
he states that formalization is necessary for progress in linguistics:

\begin{quote}
Precisely constructed models for linguistic structure can play an
important role, both negative and positive, in the process of discovery 
itself. By pushing a precise but inadequate formulation to
an unacceptable conclusion, we can often expose the exact source
of this inadequacy and, consequently, gain a deeper understanding
of the linguistic data. More positively, a formalized theory may 
automatically provide solutions for many problems other than those
for which it was explicitly designed. Obscure and intuition-bound
notions can neither lead to absurd conclusions nor provide new and
correct ones, and hence they fail to be useful in two important respects. 
I think that some of those linguists who have questioned
the value of precise and technical development of linguistic theory
have failed to recognize the productive potential in the method
of rigorously stating a proposed theory and applying it strictly to
linguistic material with no attempt to avoid unacceptable conclusions by ad hoc adjustments or loose formulation.
\citep[p.\,5]{Chomsky57a}
\end{quote}

In a book that appeared some years later, Manfred Bierwisch argued for machine processable
implementations of theoretical analyses:\footnote{
It is well possible that the rules that we formulated generate
sentences which are outside of the set of grammatical sentences in an
unpredictable way, that is, they violate grammaticality due to
properties that we did not exclude deliberately in our examination. This
is meant by the statement that a grammar is a hypothesis about the
structure of a language. A systematic check of the implications of a
grammar that is appropriate for natural languages is surely a task that
cannot be done by hand any more. This task could be solved by
implementing the grammar as a calculating task on a computer so that it
becomes possible to verify to which degree the result deviates from the
language to be described.}
\begin{quote}
Es ist also sehr wohl möglich, daß mit den formulierten Regeln Sätze erzeugt werden können,
die auch in einer nicht vorausgesehenen Weise aus der Menge der grammatisch richtigen Sätze herausfallen,
die also durch Eigenschaften gegen die Grammatikalität verstoßen, die wir nicht wissentlich aus
der Untersuchung ausgeschlossen haben. Das ist der Sinn der Feststellung, daß eine Grammatik
eine Hypothese über die Struktur einer Sprache ist. Eine systematische Überprüfung der Implikationen
einer für natürliche Sprachen angemessenen Grammatik ist sicherlich eine mit Hand nicht mehr
zu bewältigende Auf"|gabe. Sie könnte vorgenommen werden, indem die Grammatik als Rechenprogramm in einem
Elektronenrechner realisiert wird, so daß überprüft werden kann, in welchem Maße das Resultat
von der zu beschreibenden Sprache abweicht. \citep*[\page 163]{Bierwisch63}
\end{quote}
We wholehartedly agree with Bierwisch's statement, given that after the time of his writing
enourmous headway was made both empirically and theoretically. For instance, \citet{Ross67}
discovered constraints on reordering constituents and nonlocal dependencies, \citet{Perlmutter78}
discovered unaccusative verbs in the 70ies. We got theories regarding case
\citep*{YMJ87,Meurers99b,Prze99}, and verbal complex formation\is{verbal complex} and partial
fronting (\citealp{Evers75a,Grewendorf88,HN94a,Kiss95a}; G.\ \citealp{GMueller98a};
\citealp{Meurers99c}; \citealp{Mueller99a,Mueller2002b}; \citealp{deKuthy2002a}). All these
phenomena and a lot more phenomena interact!

To emphazize this point we give another quote from Steve Abney who worked within the GB framework:
\begin{quote}
A goal of earlier linguistic work, and one that is still a central goal of the
linguistic work that goes on in computational linguistics, is to develop grammars
that assign a reasonable syntactic structure to every sentence of English,
or as nearly every sentence as possible. This is not a goal that is currently much
in fashion in theoretical linguistics. Especially in Government-Binding theory
(GB), the development of large fragments has long since been abandoned in
favor of the pursuit of deep principles of grammar.
The scope of the problem of identifying the correct parse cannot be appreciated
by examining behavior on small fragments, however deeply analyzed.
Large fragments are not just small fragments several times over---there is a
qualitative change when one begins studying large fragments. As the range of
constructions that the grammar accommodates increases, the number of undesired parses for sentences
increases dramatically. \citep[\page 20]{Abney96a}
\end{quote}


\noindent
In addition it is a goal of much current linguistic theorizing to formulate constraints that hold for
all langauges or at least for certain classes of languages. As a consequence, it is not sufficient
to study the interaction between phenomena solely on the basis of one language: changing the
constraints for a certain phenomenon in one language may be compatible with all phenomena that are
relevant for the language under discussion, but it may well be that we find unexpected interactions
with other phenomena in another language. Verifying the consequences of a simple change of a
principle therefore results in a complexity that cannot be handled by human beings. It is therefore
necessary to formalize the theories in a way that makes them implementable as compter processable
grammars. After checking the grammar for consistency, a computer grammar can be used to analyze
systematically constructed test suites containing thousands of grammatical sentences and
ungrammatical word sequences or large corpora containing naturally occuring data. Such parses can be
used to verify that the grammar makes the right predictions as far as the empirical facts are concerned
(\citealp[Kapitel~22]{Mueller99a}; \citealp{OF98}; \citealp{Bender2008c}, \citealp[Section~3.7.2]{MuellerGTBuch1}).

After more than 55 years of work in transformational grammar one has to note that there are no large
scale implemented fragments on the basis of transformational analyses. Chomsky made important
contributions to the theory of formal languages that are still relevant in computer science
\citep{Chomsky59a-u}, but in 1981 he turned his back on pricecely worked out solutions:
\begin{quote}
%It is this point of view that lies behind the rough distinction between leading ideas and execution, 
%and that motivates much of what follows. 
I think that we are, in fact, beginning to approach a grasp of certain 
basic principles of grammar at what may be the appropriate level of abstraction. At the same time, 
it is necessary to investigate them and determine their empirical adequacy by developing quite specific mechanisms.
We should, then, try to distinguish as clearly as we can between discussion that bears on leading ideas and
discussion that bears on the choice of specific realizations of them. \citep*[\page 2--3]{Chomsky81a}
\end{quote}
He made it explicit in a letter to \emph{Natural Language and Linguistic Theory}:
\begin{quote}
Even in mathematics, the concept of formalization in our sense was not
developed until a century ago, when it became important for advancing research
and understanding. I know of no reason to suppose that linguistics is so much
more advanced than 19th century mathematics or contemporary molecular
biology that pursuit of Pullum's injunction would be helpful, but if that can be
shown, fine. For the present, there is lively interchange and exciting progress
without any sign, to my knowledge, of problems related to the level of formality
of ongoing work. \citep[\page 146]{Chomsky90a}
\end{quote}
The consequence of this change was that there is a huge amount of publications in Mainstream
Generative Grammar but that many of them make incompatible assumptions so that it is not clear how
insights from different publications can be combined.
An example of this is that there are many different definitions of the rather central concept of
government (see \citew{AS83a} for an overview).

This was repeatedly critizised in the 80ies for instance by the practicioners of GPSG (\citealp*[\page 6]{GKPS85a};
\citealp{Pullum85a,Pullum89b}; \citealp[\page 48]{Pullum91b}; \citealp{KP90a}). 
%
The lack in precision, missing details\footnote{
  See for instance \citew[\page 550]{Kuhns86a}, \citew[\page 508]{CL92a}, \citew[\page262]{KT91a}, \citew[\page
  3]{Kolb97a}, and \citew[\page 580]{Freidin97a-u}, \citew[\page 25, 47]{Veenstra98a}, \citew[\page 888]{LLJ2000b}, and
  \citew[\page 397, 399, 400]{Stabler2010a} regarding the latter point. 
} and frequent changes in the basic assumptions\footnote{
  See for instance \citew[\page 4]{Kolb97a}, \citew{Fanselow2009a} and the quote by Stabler below.
} resulted in the absence of large scale computer implementations that incorporate insights from
Mainstream Generative Grammar. There are some implementations that borrow from GB/MP models or from
ideas from Mainstream Generative Grammar
\citep{Petrick65a-u,ZFHW65a,Friedman69a,FBDPM71a-u,Morin73a-u,Marcus80a-u,AC86a,Kuhns86a,Correra87a,Stabler87a,Stabler92a-u,Stabler2001a,KT91a,Fong91a-u,CL92a,Lohnstein93a-u,FC94a,Nordgard94a,Veenstra98a},
but these implementations usually do not employ transformations or deviate in other crucial ways
from theoretical work. See \citew[\page 10]{Kay2011a} for discussion of early transformational
systems and \citew[Section~3.7.2]{MuellerGTBuch2} for further discussion of GB and Minimalist systems.
% Kuhns87a:549

There are two implementations that can be seen as implementations of Minimalist ideas. I will
comment on them briefly: \citet{Stabler2001a} shows how Kayne's theory of remnant movement can be
formalized and implemented. However, his implementation does not use transderivational constraints,
does not have numerations, no Agree (see \citew{Fong2012a}) and so on. Stabler's grammars are
small-scale fragments that can be considered as Prof of Concept but nothing more. They only deal
with syntax. There is no morphology\footnote{ 
The test sentences have the form in (i).
\eal
\ex the king will -s eat
\ex the king have -s eat -en
\ex the king be -s eat -ing
\ex the king -s will -s have been eat -ing the pie
\zllast
}, no treatment of multiple agreement \citep[Section~27.4.3]{Stabler2010b} and no semantics. PF and
LF processes are not modeled.\footnote{
    See for instance \citet{SE2002a} for suggestions on PF and LF movement that includes the
    deletions of parts of copies (p.\,285). The implementation of such analyses is probably non-trivial.
} 

The grammars and the processing system developed by Sandiway Fong \citep{FG2012a,Fong2012a} have a
similar status: the grammar fragemnts are small, encode syntactic aspects like Labeling directly in
the phrase structure rules \citep[Abschnitt~4]{FG2012a} and hence fall way back behind \xbart. The
grammars do not have a morphology component. Spell-Out is not implemented and hence this system
neither parses nor generates a single sentence from any natural language.\footnote{
  The following claim by \citet*[\page 1221]{BPYC2011a} is therefore simply wrong: \emph{But since we have
    sometimes adverted to computational considerations, as with the ability to ``check'' features of
    a head/label, this raises a legitimate concern about whether our framework is computationally
    realizable. So it is worth noting that the copy conception of movement, along with the locally
    oriented ``search and labeling'' procedure described above, can be implemented computationally
    as an efficient parser; see Fong, 2011, for details.} One cannot claim that one has an efficient
  implementation if the software under consideration does not parse any sentence at all, since it
  might be possible that the implementation of the missing parts is extremely complex and the
  resulting program would be inefficient. As was noted above, Fong does not implement Labeling as it
  was suggested in Chomsky's papers (for instance \citew{Chomsky2008a,Chomsky2013a}), but simply
  uses definite clause grammars. In fact neither of the two Chomsky papers is implementable, since the
  description of Labeling is not worked out in detail. Crucial cases are missing in the 2008 paper
  and the 2013 paper is vague in some places and inconsistent in others \citep{MuellerUnifying}.
}

The reason for missing large scale fragments in the framework of GB/MP is probably that the basic
assumptions that are made in the Minimalist community are changing very frequently:
\begin{quote}
\label{Zitat-Stabler}
In Minimalism, the triggering head is often called a \emph{probe}, the moving element is called a
\emph{goal}, and there are various proposals about the relations among the features that trigger
syntactic effects. \citet[p.\,229]{Chomsky95a-u} begins with the assumption that features represent
requirements which are checked and deleted when the requirement is met. The first assumption is
modified almost immediately so that only a proper subset of the features, namely the `formal',
`uninterpretable' features are deleted by checking operations in a successful derivation (Collins,
1997; \citealp[§4.5]{Chomsky95a-u}). Another idea is that certain features, in particular the
features of certain functional categories, may be initially unvalued, becoming valued by entering
into appropriate structural configurations with other elements (\citealp{Chomsky2008a}; Hiraiwa,
2005). And some recent work adopts the view that features are never deleted
\citep[p.\,11]{Chomsky2007a}. These issues remain unsolved. \citep[\page 397]{Stabler2010a} 
\end{quote}
Developing a grammar fragment takes at least three years. Large grammars accumulate the knowledge of
several researchers which cristalized in international cooperations in several years or even
decades. However, such a process is blocked when basic assumtions are changed frequently (see also
\citew[\page 138]{Fanselow2009a}).


The same criticism that applies to GB/""Minimalism\indexgb\indexminimalism applies to Construction
Grammar: The basic notions and key concepts are hardly ever made explicit with the exception of
Sign-Based Construction Grammar\indexsbcxg \citep{Sag2010b,Sag2012a}, which is an HPSG-variant,
Embodied Construction Grammar\indexecxg \citep{BC2005a}, which uses feature value matrices and is
equivalent to HPSG (see \citew[Chapter~9.6]{MuellerGTBuch1} for the discussion of both
theories), and Fluid Construction Grammar \citep{SteelsFluid-ed}. 



\section{The Poverty of the Stimulus and Motivation of Analyses}
\label{sec-pos-motivation}

In this section, I first describe recent advances in research on language acquisition and then show
how the data-driven bottom-up approach to theory development that is followed in the CoreGram
project looks like.

\subsection{Language Acquisition and Linguistic Theory}

As\is{language acquisition|(}\is{Universal Grammar (UG)|(} is argued in \citew[Chapter~11.4]{MuellerGTBuch1}, HPSG is compatible
with UG-based models of language acquisition as for instance the one by \citet{Fodor98b}. See
\citew[\page 385]{Fodor2001a} for an explicit remark to that end. However, in recent years evidence
has accumulated that arguments for innate language specific knowledge are very weak. For instance,
\citet{Johnson2004a} showed that Gold's proof that natural langauges are not identifiable in the
limit by positive data alone \citep{Gold67a} is irrelevant for discussions of human language
acquisition. Furthermore, there is evidence that the input that humans have is sufficiently rich to
aquire structures which were thought by Chomsky (\citeyear[\page 29--33]{Chomsky71a-u};
\citeyear[\page 39]{Chomsky2013a}) and others to be inacquirable: \citet{Bod2009a} showed how syntactic
structures could be derived from an unannotated corpus by Unsupervised Data-Oriented Parsing. He
explained how Chomsky's auxiliary inversion data can be captured even if the input does not contain
the data that Chomsky claims to be necessary (see also \citew{Eisenberg92b} and
\citew{PS2002a,SP2002b} for other Poverty of the Stimulus\is{Poverty of the Stimulus}
arguments). Input-based models of language acquisition in the spirit of 
\citet{Tomasello2003a} seem highly promising and in fact can explain language acquisition data
better than previous UG-based models \citep{FPG2006a,FPG2009a}. We argued in \citew{MuellerGTBuch1}
that the results from language acquistion reasearch in the Construction Grammar\indexcxg framework can be
carried over to HPSG, even in its lexical variants.\footnote{%
  In fact we believe that a lexical treatment of argument structure is the only one that is
  compatible with the basic tenets of theories like Categorial Grammar (CG)\indexcg, Lexical
  Functional Grammar (LFG)\indexlfg, CxG\indexcxg, and HPSG that adhere to lexical integrity
  \citep{BM95a}. For discussion see \citew{Mueller2006d}, \citew[Chapter~11.11]{MuellerGTBuch1},
  \citew{MuellerPersian}, \citew{MuellerUnifying}, and \citew{MWArgSt}.  
} 
If language acquisition is input"=based and language-specific innate knowledge is minimal as assumed
by \citet{Chomsky95a-u} and \citet*{HCF2002a} or non-existing, this has important consequences for the
construction of linguistic theories: Proposals that assume more than 400 morpho"=syntactic categories that
are all innate and that play a role in all languages of the world even though they are not directly
observable in many languages \citep[\page 55, 57]{CR2010a} have to be rejected right away. Furthermore, it cannot
be argued for empty functional projections in language X on the basis of overt morphems in language
Y. This has been done for Topic projections that are assumed for languages without topic morphemes
on the basis of the existence of a topic morpheme in Japanese\il{Japanese} and Gungbe. Similarly, functional
projections for object agreement (AgrO) have been proposed for languages like English\il{English} and
German\il{German} on the basis of Basque\il{Basque} data even though neither English nor German 
has object agreement. Since German children do not have any evidence from Basque, they would not be
able to acquire that there are projections for object agreement and hence this fact would have to be
known in advance. Neither can the existence of postpositions and agreement in Hungarian be seen as
evidence for AgrO projections and hidden movement processes in English as assumed in the analysis by
\citet*[\page 124]{HNG2005a}. Such complicated analyses cannot be motivated language internally and
hence are not acquirable from input alone. Since there is no theory external evidence for such
projections, theories that can do without such projections and without stipulations about UG should be preferred. 

However, this
does not mean that the search for universals or for similarities between languages and language
classes is fundamentally misguided, although it may be possible that there is very little that is
truly universal \citep{EL2009a}:\footnote{
  But see \citew{Harbour2011a} and the responses of authors in the same volume as Evans and
  Levinson's contribution for criticism of this paper.
} In principle there exist infinitely many descriptions of a
particular language. We can write a grammar that is descriptively adaquate, but the way the grammar
is written does not extend to other languages. So even without making broad claims about all
languages it is useful to look at several languages and the more they differ from each other the
better it is. What we try to do in the CoreGram project is the
modest version of main stream generative grammar: We start with grammars of individual languages and
generalize from there. We think that the framework we are using is well-suited for capturing
generalizations within a language and across languages, since inheritance hierachies are ideal tools
for this. Note though that inheritance hierarchies are not the only place in the theory where
generalizations can be captured. This is discussed in more detail in the Sections~\ref{sec-data-driven-bu} and \ref{sec-complex-antecendents}.
%(see Section~\ref{sec-coregram-setup}).  
Of course when building grammars we can
rely on several decades of research in theoretical linguistics and build on insights that were found
by researchers working under UG-oriented assumptions. Without a theory-driven comparative look at
language certain questions never would have been asked and it is good that we have such valuable
resources at hand although we see some developments rather critical as should be clear from the
statements we made above. 



\subsection{Data-Driven, Bottom-Up Theory Development}
\label{sec-data-driven-bu}

Instead of imposing constraints from one language onto other languages, a bottom-up approach seems
to be more appropriate: Grammars for individual languages should be motivated language
internally. Grammars that share certain properties can be grouped in classes. This makes it possible
to capture generalizations about groups of languages and language as such. Let us consider some
examples: German, Dutch, Danish, English and French. If we start developing grammars for German and
Dutch, we find that they share a lot of properties: both are SOV and V2 languages, both have a
verbal complex. One main difference is the order of elements in the verbal complex. The situation
can be depicted as in Figure~\vref{fig-german-dutch}.
\begin{figure}[htbp]
\centering
\begin{tikzpicture}
    \tikzset{level 1+/.style={level distance=5\baselineskip}}%
    \tikzset{sibling distance=18pt}
%    \tikzset{frontier/.style={distance from root=10\baselineskip}}%
    \tikzset{every tree node/.style={
                          %  The shape:
                          rectangle,minimum size=6mm,rounded corners=3mm,
                          %  The rest
                          very thick,draw=black!50,
                          top color=white,bottom color=black!20,
                          font=\ttfamily},node distance=2mm}
    \Tree[.\node (Set3) { ~Set 3~ };
                  \node (Set1) { ~Set 1~ }; \node (Set2) { ~Set 2~ }; ]  

    \node [below=of Set1] {German}; \node [below=of Set2] {Dutch}; 

%    \node [left=of Set5] {V2};
     \node [left=of Set3] {\begin{tabular}{@{}c@{}}Arg St\\V2\\SOV\\VC\end{tabular}};
%    \node [right=of Set11] {SVO};

     \end{tikzpicture}
\caption{\label{fig-german-dutch}Shared properties of German and Dutch}
\end{figure}
There are some properties that are shared between German and Dutch (Set 3). For instance, the
argument structure, a list containing descriptions of syntactic and semantic properties of
arguments, and the linking of these arguments to the meaning of the sign is contained in Set 3. In
addition the constraints for SOV languages, the verb position in V2 clauses and the fronting of a
constituent in V2 clauses is contained in Set 3. The respective constraints are shared between the two grammars. When we
add another language, say Danish, we get further differences. While German and Dutch are SOV, Danish
is an SVO language. Figure~\vref{fig-german-dutch-danish} shows the resulting situation: The
top-most node represents constraints that hold for all languages (for instance the argument
structure constraints, linking and V2) and the node below it (Set~4) contains
constraints that hold for German and Dutch only.\footnote{
  In principle, there could be constraints that hold for Dutch and Danish but not for German and for
  German and Danish, but not for Dutch. These constraints would be removed from Set 1 and Set 2
  respectively and put into another constraint set higher up in the hierarchy. These sets are not
  illustrated in the figure and we keep the names Set~1 and Set~2 from Figure~\ref{fig-german-dutch} for the constraint sets for German
  and Dutch.
} For instance Set~4 contains constraints regarding verbal complexes and SOV order.
\begin{figure}[htbp]
\centering
\begin{tikzpicture}
    \tikzset{level 1+/.style={level distance=5\baselineskip}}%
    \tikzset{sibling distance=18pt}
    \tikzset{frontier/.style={distance from root=10\baselineskip}}%
    \tikzset{every tree node/.style={
                          %  The shape:
                          rectangle,minimum size=6mm,rounded corners=3mm,
                          %  The rest
                          very thick,draw=black!50,
                          top color=white,bottom color=black!20,
                          font=\ttfamily},node distance=2mm}
    \Tree[.\node (Set5) { ~Set 5~ };
               [.\node (Set4) { ~Set 4~ };
                  \node (Set1) { ~Set 1~ }; \node (Set2) { ~Set 2~ }; ] \node (Set6) { ~Set 6~ }; ] 

    \node [below=of Set1] {German}; \node [below=of Set2] {Dutch};  \node [below=of Set6] {Danish};

    \node [left=of Set5] {\begin{tabular}{@{}c@{}}Arg Str\\V2\end{tabular}};
    \node [left=of Set4] {\begin{tabular}{@{}c@{}}SOV\\VC\end{tabular}};
%    \node [right=of Set11] {SVO};

     \end{tikzpicture}
\caption{\label{fig-german-dutch-danish}Shared Properties of German, Dutch, and Danish}
\end{figure}
The union of Set 4 and Set 5 is the Set 3 of Figure~\ref{fig-german-dutch}.

If we add further languages, further constraint sets will be
distinguished. Figure~\vref{fig-german-dutch-danish-english-french} shows the situation that results
when we add English and French.
\begin{figure}[htbp]
\centering
\begin{tikzpicture}
    \tikzset{level 1+/.style={level distance=5\baselineskip}}%
    \tikzset{sibling distance=18pt}
    \tikzset{frontier/.style={distance from root=15\baselineskip}}%
    \tikzset{every tree node/.style={
                          %  The shape:
                          rectangle,minimum size=6mm,rounded corners=3mm,
                          %  The rest
                          very thick,draw=black!50,
                          top color=white,bottom color=black!20,
                          font=\ttfamily},node distance=2mm}
    \Tree[.\node (Set8) { ~Set 8~ };
            [.\node (Set7) { ~Set 7~ };
               [.\node (Set4) { ~Set 4~ };
                  \node (Set1) { ~Set 1~ }; \node (Set2) { ~Set 2~ }; ] \node (Set6) { ~Set 6~ }; ] 
               [.\node (Set11) { ~Set 11~ }; \node (Set12) { ~Set 12~ };  \node (Set13) { ~Set 13~ }; ] 
    ]
    \node [below=of Set1] {German}; \node [below=of Set2] {Dutch};  \node [below=of Set6] {Danish};
    \node [below=of Set12] {English}; \node [below=of Set13] {French}; 

    \node [left=of Set8] {Arg Str};
    \node [left=of Set7] {V2};
    \node [left=of Set4] {\begin{tabular}{@{}c@{}}SOV\\VC\end{tabular}};
    \node [right=of Set11] {SVO};

    \draw (Set11.south) -- (Set6.north);
    \end{tikzpicture}

\caption{\label{fig-german-dutch-danish-english-french}Languages and language classes}
\end{figure}
Again, the picture is not complete since there are constraints that are shared by Danish and English
but not by French, but the general idea should be clear: by consequently working this way, we should
arrive at constraint sets that directly correspond to those that were established in the typological
literature.

It should be clear from what has been said so far that the goal of every scientist that works this
way is to find generalizations and to describe a new language in a way that reuses theoretical constructs
that have been found useful for a language that is already covered. However, as was explained above
the resulting grammars should be motivated by data of the respective languages and not by facts from
other languages. In situations where more than one analysis would be compatible with a given dataset
for language X the evidence from language Y with similar constructs is most welcome and can be used
as evidence in favor of one of the two analyses for language X. I call this approach the
\emph{bottom-up approach with cheating}: unless there is contradicting evidence we can reuse
analyses that have been developed for other languages.  

After my talk at the MIT in 2013 members of the linguistics department objected against this
approach and claimed that it would not make any predictions as far as possible/impossible languages
are concerned. Regarding predictions two things must be said: Firstly, predictions are being made on a
language particular basis. As an example consider the following sentences from \citet{Netter91}:


\eal
\ex 
\gll {}[Versucht, zu lesen], hat er das Buch nicht.\\
       \spacebr{}tried to read has he the book not\\
\glt `He did not try to read the book.'
\ex 
\gll {}[Versucht, einen Freund vorzustellen], hat er ihr noch nie.\\
       \spacebr{}tried a friend to.introduce has he her yet never\\
\glt `He never tried to introduce her to a friend.'
\zl
When I first read them I had no idea about their structure. I switched on my computer and typed them
in and within milliseconds I got an analysis of the sentences and by inspecting the result I realized
that these sentences are combinations of partial verb phrase fronting and the so-called third
construction \citep[\page 439]{Mueller99a}. I implemented analyses of both phenomena but never thought about the interaction of
the two. The grammar predicted that examples like (\mex{0}) are grammatical. Similarly the
constraints of the grammar interact to rule out certain structures. So predictions about
ungrammaticality/impossible structures are made as well.

Secondly, the top-most constraint set holds for all languages seen so far. It can be regarded as a
hypothesis about properties that are shared by all languages. This constraint set contains
constraints about the connection between syntax and information structure and such constraints allow
for V2 languages but rule out languages with the verb in penultimate position. Of course if a
language that does this is found, a more general top-most set has to be defined, but this is
parallel for Minimalist theories: If languages are found that are incompatible with basic
assumptions, the basic assumptions have to be revised. As with the language particular constraints
the constraints from the top-most set make certain predictions about what can be and what cannot be
found in languages.

\citew[\page 106]{Cinque99a-u} suggested a cascade of functional projections to account for
reoccuring orderings in the languages of the world. He assumes elaborate tree structures to play a
role in the analysis of all sentences in all languages even if there is no evidence for respective
morphosyntactic distinctions in a particular language (see also \citew[\page 55]{CR2010a}). In the
latter case Cinque assumes that the respective tree nodes are empty. Cinques results could be
incorporated in the model advocated here. We would define part of speech categories and
morpho-syntactic features in the top-most set and state linearization constraints that enforce the
order that Cinque encoded directly in his tree structure. In languages in which such categories are
not manifested by lexical material the constraints would never apply. Neither empty elements nor
elaborate tree structures would be needed. So, we have shown that Cinques data could be
covered in a better way in an HPSG with a rich UG, but we nevertheless refrain from introducing 400 categories (or
features) into the theories of all languages, point out again that this is implausible from a
genetic point of view and wait for other probably functional explanations of the Cinque data.

Often discussed examples like languages that form questions by reversing the order of the words in a
string \citep{MMGRRBW2003a} need not be ruled out in the grammar, since they are ruled out by
language external constraints: we simply do not have enough working memory to do such complex computations.

\section{Coverage and Highlights}
\label{sec-coverage-and-highlights}

The grammars of German, Persian, and Danish are relatively big. The German grammar (BerliGram) was the first one
that was implemented. It is an extension of the grammars that were developed for the individual
chapters of the HPSG text book \citep{MuellerLehrbuch1}. The Situation Semantics that is used in the
text book was replaced by a Minimal Recursion Semantics (MRS, \citealp{CFPS2005a}). MRS allows for
underspecification of scope so that a sentence like (\mex{1}) gets one representation from which the several scopings can be derived. See
\citew[Section~5.6]{Dowty79a} on the discussion of the readings of \emph{again} in English
and \citet{Egg99a} for the explanation of the different readings of (\mex{1}).
\eal
\ex
\gll dass Max wieder alle Fenster öffnete\\
     that Max again all windows opend\\
\glt `that Max opened all windows again'
\ex \relation{again}($\forall$(CAUSE(open))); repetitiv
\ex \relation{again}(CAUSE($\forall$(open))); repetitiv
\ex CAUSE(\relation{again}($\forall$(open))); restitutiv
\zl
\Citet[\page 93]{Stechow96a} develops an analysis in the framework of Minimalism that assumes an
empty VOICE head that contributes the CAUSE relation and some further functional heads for AgrO and
AgrS. The empty heads were used to derive several readings in a movement-based analysis. However, as
\citet{JB2003a-u} pointed out, von Stechow's analysis cannot derive all readings. We therefore
follow \citet{Egg99a} and treat (\mex{0}a) as an instance of lexical subscoping: \emph{öffnen} is lexically decomposed
into CAUSE(open) and the \emph{again} can scope below the CAUSE operator although there is no
decomposition in syntax. The scope relations are represented in dominance graphs. Egg's analysis was
translated into MRS (see \citew[Section~19.9.2]{MuellerGTBuch1}).

Apart from the modification of the semantics component further special phenomena were
implemented. For instance an analysis of multiple frontings \citep{Mueller2003b}, something that is
unique among the existing HPSG implementations. For a discussion of approaches to constituent order
that are incompatible with the multiple fronting data see
\citew{Mueller2005c,MuellerGS}. Furthermore the analysis of depictives was added
\citep{Mueller2008a}. Some phenomena that have been covered in earlier grammars of German have not yet
been transfered to BerliGram.

The Danish grammar is documented in a 500+ page book, which is not complete yet. The following
examples show in a compact way the interaction of several phenomena: passive with promotion of
either the direct object or the indirect object (\mex{1}a,c), passive and
pronoun shift (\mex{1}b,d), partial fronting and object shift in (\mex{1}b,d):

%Interaktion von Passiv (Promotion des direkten und indirekten Objekts), Pronoun-Shift, Verb-Voranstellung (V1), partieller Voranstellung (V2)

\eal
\ex[]{
\gll Bjarne bliver ikke anbefalet den.\\
     Bjarne is not  recommended it\\
\glt `The book is not recommended to Bjarne.'
}
\ex[?]{
\gll Anbefalet bliver Bjarne den ikke.\\
     recommened is Bjarne it not\\
\glt `The book is not recommended to Bjarne.'
}
\ex[]{
\gll Bogen bliver ikke anbefalet ham.\\
    book.DEF is not  recommended him\\
\glt `The book is not recommended to him.'
}
\ex[?]{
\gll Anbefalet bliver bogen ham ikke.\\
    recommened is book.DEF him not\\
\glt `The book is not recommended to him.'
}
\zl
The examples in (\mex{0}b,d) are interesting since Danish differs from German and Dutch in not allowing
incomplete category fronting in general. Such partial frontings can only be found if the missing
components are shifted pronouns, that is, pronouns to the left of the negation
\citep{Holmberg99a}. Due to the complexitiy of the construction examples are marked but \citet{MOeOS} and
\citet{MOeDanish} provide attested data.

The Mandarin Chinese grammar was implemented with the help of Jia Zhongheng. We used the description
in \citew{LT81a} as part of our implementation. Among the things that are special are NPs that
contain classifiers (\mex{1}) and change of part of speech by reduplication as in (\mex{2}).
\ea
\glll   那    辆     红    的 车   锈了。  \\     
        na4   liang4 hong2 de che1 xiu4.le\\
        that CL red DE car rust.ASP\\
\glt `That red car rusts.'
\z
%
The adjective 高兴 (gao1xing4, `happy') in (\mex{1}a) is converted into an adverb by forming the pattern AABB from the original
adjective AB, that is gao1 is doubled and xing4 is doubled as well.
\eal
\ex
\glll 他  很  高兴。\\
      ta1 hen gao1xing4\\
      he very happy\\
\glt `he is very happy'
\ex
\glll   他  高高兴兴            游泳。\\
        ta1 gao1gao1xing4xing4 you3yong3\\
        he AABB=happily swims\\
\glt `He swims happily.'
\zl

The Persian grammar is a larger fragment, which needs to be fully documented \citep*{PersianBook}. A
description of some parts of the grammar can be found in \citew{MG2010a}. The grammar can be used
with Persian script or with a romanized version that is usually used in linguistic texts. The examples in (\mex{1})
show lightverb constructions, which are an important feature of the
language. (\mex{1}a) shows that the future auxiliary can interrupt the preverb verb sequence of
lightverbs. (\mex{1}b) shows an example with the negation prefix in the middle of the light verb construction and pro-drop.\NOTE{Check}
\eal
\ex
\PRL{من این کار را انجام خواهم داد.}\\
 \gll   man in kār rā anjām xāh-am dād.\footnotemark\\
        I this job DOM performance will-1SG gave\\
\footnotetext{
\citew[p.\,73]{Karimi-Doostan97a}.
}
\glt `I will do this work.'
\ex 
\PRL{مرد را دوست نداشت.}\\
 \gll   mard rā dust na-dāšt.\\
        man DOM friend NEG-had\\
\glt `He/she did not love the man.'
\zl

The Maltese grammar is an implementation of the description by \citew{Fabri93a}. Fabri works in the
framework of Lexical Decomposition Grammar, which is also a lexical framework and his analysis were
translatable into HPSG without great efforts. The examples in (\mex{1}) show definiteness
marking. (\mex{1}b) shows assimilation and (\mex{1}c) shows clitics:\footnote{
  The examples are taken from \citew[\page 130]{Fabri93a}.
}
\eal
\ex 
\gll Il-komunist xejjer lil-l-papa.\\
     DEF-communist winks(3msg) Case-DEF-pope(msg)\\
\glt `the communist winks the pope.'
\ex
\gll It-terrorist bagħat l-ittr-a   lil-l-president.\\
     DEF-Terrorist schickte        DEF-Brief-F Ks-DEF-Präsident\\
\glt `The terrorist sent the president the letter.'
\ex
\gll It-terrorist bagħat=hie=l.\\
     DEF-Terrorist send.3M.SG=3F.SG=3M.SG\\ 
\glt `The terrorist sent it to him.'\\
\zl
(\mex{0}c) is ambiguous. There is a reading with clitic left dislocation. Both readings are found by
the grammar.


\section{Basic Assumptions}
\label{sec-basic-assumptions}

\subsection{Valence}

We assume that valence is represented in a uniform way across languages.\footnote{
  \citet{KM2012a} argue for an analysis of Oneida (a Northern Iroquoian language) that does not
  include a representation of syntactic valence. If this analysis is correct, syntactic argument
  structure would not be universal, but would be characteristic for a large number of languages.
} Arguments of a head are
represented in the \argstl \citep[Chapter~9]{ps2}. They are mapped to the valence features \spr and
\comps in a language dependent fashion. For instance, English and Danish map the subject to the \spr
list and the other arguments to \comps. Danish inserts an expletive in cases in which there is no
element that can be mapped to \spr, while English does not do this \citep{MOe2013a}. German differs from both
languages in mapping all arguments of finite verbs to the \compsl \citep{Pollard90a}.

The arguments in the \argstl are ordered according to the obliqueness hierarchy of \citet{KC77a},
which plays a role in the analysis of a variety of phenomena (for instance case assignment and
depictive predicates). The elements of the \argstl are linked
to the semantic roles that a certain head has to fill. Since the traditional role labels like agent
and patient are problematic, we adopt Dowty's proto role approach \citeyearpar{Dowty91a}. We use {\sc
  arg1}, {\sc arg2}, and so on as role labels.
    

\subsection{Constituent Structure and Constituent Order}
\label{sec-const-order}

Classical HPSG came with very few immideate dominance schemata: Head"=Complement Schema,
Head-Specifier Schema, Head-Adjunct Schema, the Head-Filler Schema for binding off unbounded
dependencies, and the Head-Extra Schema for binding off clause bound nonlocal dependencies. Since
\citew{Sag97a} many HPSG analyses have a more constructional flavour, that is, specific
subconstructions of these general schemata are introduced \citep{Sag2010b}. In the CoreGram project
we stay within the old tradition of HPSG and keep the rather abstract dominance schemata. However, it
is possible to state further constraints on the respective structures. So rather than having several
very specific instances of the Head-Filler Schema, we have very few ones, for instance, for verb
second clauses and relative clauses and formulate additional implicational constraints that
constrain actual instances of head filler phrases further if the antecedent of the implicational
constraint is true. An example of such a constraint is the following one, which was suggested by
\citet[\page 75]{BC2010a}:
\ea
\label{constr-mult-front}
\ms[head-filler-phrase]{
non-head-dtrs & \liste{ head|dsl \type{local} }\\
} \impl \ms{ is & pres $\vee$ a-top-com $\vee$ \ldots }
\z
The constraint says that for all head-filler phrases that have a non-head daughter whose \dslv is of
type \type{local}, the value of the information structure feature {\sc is} has to be \type{pres} $\vee$ \type{a-top-com} $\vee$ \ldots.

Since the schemata are rather general they can be used for all languages under consideration so
far. Of course the languages differ in terms of consituent order, but this can be dealt with by
using linearization rules that are sensitive to features whose values are language specific. For
instance, all heads have a feature {\sc initial}. The value is `+', if the head has to be serialized
before its complements and `--' if it follows its complements. German and Persian verbs are \initial
$-$, while English, Danish, Mandarin Chinese and Maltese verbs are \initial +.

We assume binary branching structures and hence we get the structures in (\mex{1}) for English and
the corresponding German example:
\eal
\ex He [[gave the woman] a book].
\ex 
\gll er [der Frau [ein Buch gab]]\\
     he \spacebr{}the woman \spacebr{}a book gave\\
\zl
The LP rules enforce that \emph{gave} is linearized before \emph{the woman} and \emph{gave the
  woman} is linearized before \emph{a book}.

The scrambling of arguments is accounted for by ID schemata that allow the combination of a head
with any of its arguments independent of the position an element has in the valence list of its
head. Similar analyses have been suggested in the framework of HPSG by \citet{Gunji86a} for Japanese
and \citet{Pollard90a} for German. Many authors assume a valence set rather than a list. However, 
%as was pointed out in \citew{Mueller2004b}, 
the order of the elements has to be represented somewhere
in the grammar since it is relevant for various phenomena. In the proposal adopted in the CoreGram
project we have an ordered list but allow the saturation in an arbitrary order. For non-HPSG
analyses that are similar to the set-based approaches see \citew{Fanselow2001a} and \citew{SB2006a-u}.

Non-scrambling languages like English combine heads with their complements
in a strict order: the least oblique element is combined with the head first and then the more
oblique complements follow. Non-scrambling languages with head-final order take the last element
from the valence list first. Again see \citew{SB2006a-u} for a similar proposal in the framework of
Categorial Grammar.

\subsection{Morphology and Lexical Rules}

We follow a lexical rule based approach to morphology. Lexical rules are basically unary branching
trees that license new lexical items.\footnote{
  \citet{Goldberg2012a} calls such lexical rules \emph{lexical templates} and sets them apart from
  lexical rules that relate stored lexical items.
}
A lexical rule can add or subtract to the phonology (in
implementations the orthography) of an input item. For instance, it is possible to analyze the
complex morphological patterns that we observe in semitic languages by mapping a root
consisting of consonants to a full-fledged stem or word that has the appropriate vowels inserted.
We follow \citet{BM95a} in assuming the Lexical Integrity Principle. This means that all
morphological combinations have to be done by lexical rules, that is, fully inflected forms are part
of the lexicon, most of them being licensed by productive lexical rules.

Lexical rules do not have to change the phonology/orthography of the item they apply to. For
instance lexical rules can be used to extend the valence of lexical items. As was argued in
\citew{Mueller2002b,Mueller2006d} resultative constructions should be treated lexically. So there is
a lexical rule that maps the stem \stem{fisch} of the intransitive version of the verb
\emph{fischen} (`to fish') onto a stem \stem{fisch} that selects for a secondary predicate
(adjective or PP) and the subject of this predicate.
\ea
\gll Er fischt den Teich leer.\\
     he fishes the pond empty\\
\z 

\subsection{Semantics}

All grammars come with a semantics component. We use Minimal Recursion Semantics \citep{CFPS2005a} since it allows for
the underspecification of scope (see Section~\ref{sec-coverage-and-highlights}). For instance the
sentence in (\mex{1}) has two readings: one in wich the existential quantifier outscopes the
universal quantifier and one reading in which the scopings are reversed.
\ea
\gll Jeder Sohn eines Beamten rennt.\\
     every son  of.a state.employee runs\\
\z
See the paper cited above for a discussion of the respective English example.
 
\subsection{Information Structure}

The German grammar contains constraints on information structure. The respective theory was
developed by Felix Bildhauer and Philippa Cook and implemented by Felix Bildhauer in the project A6
of the SFB 632 on information structure \citep*{BC2010a,MBC2012a}. See also (\ref{constr-mult-front})
above. The project A6 explored the various contexts for so-called multiple frontings and implemented
an analysis that refers to syntactic configurations and assigns the elementary predications from an
MRS representation to {\sc topic} and {\sc focus} lists.

Elodie Winckel currently augments the French grammar by an information structure component. This
work is also carried out in the context of the SFB and one goal of this work is to test to what
extend it is possible to explain island constraints with respect to information structure \citep{AG2008a}.

Bildhauer's Spanish grammar \citeyearpar{Bildhauer2008a} also implements a theory of information structure 
and as was mentioned above, this grammar will be ported to the CoreGram format.

\section{Implementation Details}
\label{sec-implementation-details}

\subsection{TRALE}

The grammars are implemented in TRALE \citep*{MPR2002a-u,Penn2004a-u}. TRALE implements typed
feature descriptions. Every grammar consists of a signature (a type hierarchy with feature
introduction and appropriateness constraints) and a theory that states constraints on objects of
these types. TRALE is implemented in Prolog and comes with an implementation of relational
constraints that maps the TRALE relations to Prolog relations. TRALE has two parsers: a standard
bottom-up chart parser and a linearization parser \citep{Suhre99b-u}. The CoreGram project uses the
standard bottom-up parser. Both parsers use a phrase structure backbone.
TRALE is available as bootable CD-rom \citep{Mueller2007b}. The CD-rom contains a full
installation of all components that were available in 2007. This includes a chart display for
developing and debugging grammars, Utool for visualizing dominance graphs fro semantic
representations and scope resolving \citep{KT2005a-u}, and \tsdb for systematic testing (see
Section~\ref{sec-evaluation}). We hope to finish a new virtual machine soon that includes a new and
much faster version of TRALE, the most recent versions of the CoreGram grammars, and Kahina a
powerful debugger for constraint resolution systems like TRALE \citep*{DER2010a-u}.\NOTE{reference?}

Compared to other systems like the LKB \citep{Copestake2002a} the expressive power of the
description language is high (see also \citew{MelnikHandWritten}). This allows for the rather direct implementation of analyses that are
proposed by theoretical linguists. The following descriptive devices are used in the theory and are
provided by TRALE. The references point to papers which argue for such constructs.


\begin{itemize}
\item empty elements (\citew{Kiss95a,Meurers99a,MuellerLehrbuch1,LH2006a}; \citealp{Bender2000a} und
  \citealp*[\page 464]{SWB2003a}; \citealp[Section~3.3]{Borsley2004a}; \citealp{Mueller2004e})
\item relational constraints \citep{ps2,BMS2001a,MdKM2003a},
\item complex antecedents in implicational constraints (\citealp[\page207]{Meurers2000b};
  \citealp[\page 145, 149]{KD2004a}; \citealp[\page 145, Section~10.3]{MuellerLehrbuch1}; Bildhauer and Cook \citeyear[\page 75]{BC2010a}),
\item cyclic structures (\citew[\page 56]{EV94a}; Meurers, \citeyear[\page 2007]{Meurers2000b}; \citeyear[\page 176]{Meurers2001a}, \citew[\page 638]{Samvelian2007a}), 
\item macros, and
\item a morphology component that has the expressive power that is needed to account for nontrivial
  morphological phenomena.
\end{itemize}


\subsubsection{Empty Elements}

All CoreGram grammars use empty elements to account for extraction phenomena. Auxiliary inversion in English
has not yet been implemented, but German and Danish use head-movement analyses to account for the
verb in initial position in questions and V2 clauses. TRALE has mechanisms to precompile grammars
and to eliminate most of the empty elements.

\subsubsection{Complex Antecedents}
\label{sec-complex-antecendents}

To see how useful implicational constraints with complex antecedents are both from a theoretical and
an implementational perspective, consider the constraint in (\ref{constr-mult-front})
again. Proposals that do not make use of such implementational constraints would have to introduce
two subtypes of \type{head-filler-phrase}: one for head-filler phrases with
the \dslv \type{local} -- let us call this type \type{head-filler-phrase-dsl} -- and one for head-filler phrases with the \dslv different from
\type{local}. The information structure constraints from (\ref{constr-mult-front}) will be
constraints on structures of type \type{head-filler-phrase-dsl}:
\ea
\type{head-filler-phrase-dsl} \impl \ms{ is & pres $\vee$ a-top-com $\vee$ \ldots }
\z
Proponents of such a theory would basically make explicit which daughters could appear
in the filler position. 

In our setting with implicational constraints we do not need these two
additional types. We formulate the constraint in (\ref{constr-mult-front}) and this constraint applies only to those head-filler
phrases that have a non-head daughter with a \dslv of type \type{local}. Therefore our theory is
simpler and has to be prefered over other approaches that dulicate information about the
combinatorics of linguistic objects in type names.


\subsubsection{Relational Constraints}

The relational constraint that is used most often in HPSG is \emph{append} (`$\oplus$'), which concatenates two
lists. While many of the uses of \emph{append} can be recoded using difference lists, this is not always
the case. See \citet*{MdKM2003a} for some discussion. In the implementation of scrambling that was
sketched in Section~\ref{sec-const-order} above a valence list is split into three parts. The first is a list of
arbitrary length the second is a list containing the element that has to be combined with the head
and the third is a list of arbitrary length again. This can be implemented directly using \emph{append}: A
$\oplus$ \sliste{ XP } $\oplus$ B.

Another application of relational constraints is the determination of the last element of a
list. For technical reasons the argument structure and valence lists are represented with the most
oblique element at the beginning (as in \citew{ps}). If one wants to access the least oblique
element in the \argstl, one has to find the last element of this list. In the theory of
\citet{HM94a}, which follows \citet{Haider86}, transitive and unergative verbs have a designated
argument that is identical to the least oblique argument of the verb. (\mex{1}) shows how this can
be expressed in TRALE:
\ea
\begin{tabular}[t]{@{}l@{}l@{}}
(synsem:loc:cat:(&head:da:[last(ArgSt)],\\
                 &arg\_st:ArgSt))\\
\end{tabular}
\z 
ArgSt is the value of the arg\_st feature and last(ArgSt) returns the last element of this list,
which is represented as the element of the list which is the value of the feature {\sc da}. Note
that this works for lists of arbitrary length. This is important for verbs like \emph{lassen} (`to
let') that raise the arguments of the verbal element that they embed. TRALE uses delay mechanism to
delay the execution of relational constraints until enough information is available. In the case of
our example the constraints are delayed until \emph{lassen} is combined with the embedded verb and
the actual length of the argument structure of \emph{lassen} is known.

\subsubsection{Macros}

Like types macros can be organized in hierarchies. The type hierarchies are stated in a signature
but the macro hierarchies are constructed by calling other macros in a macro definition. Macros
differ from types in allowing parameters. This makes it possible to represent the lexicon in a
rather compact way. For instance, (\mex{1}a) shows the lexical item for \emph{work} and (\mex{1}b)
the definition of the macro that is called.
\eal
\ex \verb+work  ~~> @intrans_verb(a_ work,agentive).+
\ex 
\begin{verbatim}
intrans_verb(Relation,Sort) :=
 (@strict_intrans_nerg_verb,
  rels:[(pred:Relation,
         arg1:sort:Sort)]).
\end{verbatim}
\zl 

\subsubsection{Lexical Rules and Morphology}

There is a special syntax for lexical rules in TRALE: an identifier is followed by two `\#', by the
input description, the arrow `**>' and the output description. Since we want to be as close as
possible to HPSG analyses, we assume that every lexical rule has a specific type
(\type{definiteness\_lr} in the example below). The respective typed feature structure models a
linguistic object with a daughters list. The daughter is the input of the lexical rule. In addition
to this a lexical rule has to have a morphs statement that says something on how the orthography of
the input is related to the orthography of the output. (\ref{lr-definiteness}) shows the lexical rule that is used
to account for definiteness marking in Maltese.
Definiteness is marked with an /l/ at nouns and adjectives in Maltese. (\mex{1}) gives an
example:\footnote{
  (\mex{1}), (\mex{2}) and (\mex{3}b) are underlying forms. If the definite form of \emph{book} is used
  in isolation, an /i/ has to be added. 
}
      \ea
      \gll l-ktieb\\
           {\sc def}-book\\
      \z
If the noun starts with one of the coronals /d/, /t/, /s/, /z/, /ʃ/, /ts/, /tʃ/, /n/, or /r/ the
/l/ is assimilated. (\mex{1}) gives an example:
      \ea
      \gll r-raġel\\
           {\sc def}-man\\
      \z
The only exception is the coronal /dʒ/, which is exempt from assimilation.

Inner epenthesis can be observed if the word starts with /s/ or /ʃ/ followed by a consonant:
      \eal
      \ex 
      \gll skola\\
           school\\
      \ex
      \gll l-iskola\\
           {\sc def}-school\\
      \zl
If inner epenthesis applies, it prevents assimilation.

The lexical rule splits the input characters into an initial part S and a part X and tests whether S
is [s] or [x] by calling the predicate s\_sh. If this call succeeds \emph{li} is appended to the
output. If this call does not succeed, other clauses are tried. If the input starts with a coronal,
the coronal is doubled. Otherwise the input is prefixed with an \emph{l}.
\ea
\label{lr-definiteness}
\begin{verbatim}
definiteness_lr ##
 Dtr
**>
(definiteness_lr,
 dtrs:[Dtr])
morphs
  (S,X) becomes (li,S,X)  when s_sh(S),     % l-iskola 
  (C,X) becomes (C,C,X) when coronal(C),    % r-raġel  
  X becomes (l,X).                          % l-ktieb          

s_sh([s]).
s_sh([x]).
     
coronal([d]).
coronal([t]).
coronal([s]).
coronal([z]).
coronal([n]).
coronal([r]).
\end{verbatim}
\z

\subsection{Set Up of CoreGram}
%\label{sec-coregram-setup}

The grammars are organized in one directory for every language. The respective directories contain a
subdirectory named Core-Grammar. This directory contains files that are shared between the
grammars. For instance the file core-macros.pl contains macros that are or can be used by all
languages. For every language there is a load file that loads the files from the core grammar
directory that are relevant for the respective language. So, for instance english.pl, french.pl, and
danish.pl all load nom-acc.pl since these languages are nominative-accusative languages. These files
also contain code for loading macros and constraints for languages that do not form a verbal
complex, while german.pl does load the files for cluster-forming languages. These files directly
correspond to the constraint sets that were discussed in Section~\ref{sec-pos-motivation}.

The possibility to specify type constraints makes it possible to specify constraints that hold for a
certain construction cross-linguistically in a file that is loaded by all grammars and restrict
structures of this type further in language particular files. 

Lexical rules are also described by feature descriptions and organized in type hierarchies
\citep{Meurers2001a}. Like other constraints the constraints on lexical rules can be shared. 
%% So for
%% instance Danish, English, and German use the same lexical rule for licencing attributive participles:
%% adjectives as in (\mex{1}):
%% \eal
%% \ex  

%% As was described


\section{Comparison to Other Multilingual Projects}
\label{sec-comparison}

Currently there are two large groups working in the area of multi"=lingual grammar engineering. We
will deal with both of them in the following subsections and will explain in what way the CoreGram
project differs from them. The DELPH-IN consortium\footnote{
  DELPH-IN is an abbreviation for Deep Linguistic Processing with HPSG.
}
will be dealt with in Section~\ref{sec-delphin} and ParGram in Section~\ref{sec-pargram}.


\subsection{DELPH-IN and the LinGO Grammar Matrix}
\label{sec-delphin}

The DELPH-IN group uses the LKB system for grammar development (Copestake, \citeyear{Copestake2002a}). The LinGO
Grammar Matrix provides a collection of types for lexical objects and phrasal schemata 
that can be used by grammar writers \citep*{BFO2002a-u,BF2005a-u}. The Matrix builds on experiences
from the development of grammars for English, German, Japanese, and Spanish. The Grammar Matrix
provides a starter set. This can be modified and extended by individual grammar writers without any
interaction with the other grammars. Of course there is a feedback loop: grammar writers can inform
the developers of the Grammar Matrix about their requirements and changes which they believe to be
appropriate. In the CoreGram project all grammars use the same core files. If the grammar core is
changed because of evidence in, say, Persian, all other grammars have to be compatible with the
change or have to be adapted. While this increases the complexity of the development process
considerably, the result is a consistent set of grammars for several typologically diverse
languages.

Work that is done in the DELPH-IN consortium has a strong focus on applications. Efficient
processing has a high priority. This was one cause for the reduction of the expressive power of the
description formalism and is also reflected in analyses. See for instance \citew{Crysmann2003b} and
the criticism in \citew[Section~3.6]{Mueller2005c}. In our project we see processing issues as secondary and want
to treat computationally expensive but linguistically interesting phenomena as well as those that
can be handled efficiently. Although the development of linguistically motivated analyses has the
highest priority, processability is not ignored completely. Since profiling tools (chart display and
the test suite tool [incr TSDB()] \citep{OC2000b-u}) are integrated into TRALE, an exact examination
of the grammars is possible and unnecessary computations of the parser can be detected and
eliminated.


\subsection{ParGram}
\label{sec-pargram}


A similar community works in the framework of LFG and is organized in the ParGram project
\citep*{BKNS99a-ed,BDKMR02a-u}. The goal of the project is the implementation of parallel LFG
grammars for a set of languages. Until now there exist grammars for Arabic, Danish, Georgian,
Japanese, Malagasy, Norwegian, Tigrinya, Turkish, Hungarian, Urdu, Vietnamese, and Welsh (see
\citew[Chapter~6]{MuellerGTBuch2} for an overview and references).\NOTE{mehr?} These
grammars are parellel in that they produce f"=structures that have the same feature geometry and
uniform analyses of the phenomena. Parallel grammar development is challenging for developers, since
each phenomenon has to be examined carefully and it has to be decided whether a cross"=linguistic
analysis is feasible at all or whether the phenomenon is idiosyncratic and language specific or
whether the feature geometry has to be changed. The grammars are developed in the X(erox)
L(inguistic) E(nvironment) system \citep*{KKM2002a-u,BKNS99a-ed}.

The ambition of our project (and also the Grammar Matrix) is higher than the one of the ParGram
project, since the HPSG grammars model the whole range of grammatical properties: morphology,
syntax, semantics, and information structure are described with the same feature geometry.
Dominance schemata for head"=argument phrases, head"=adjunct phrases, filler"=head phrases,
specifier"=head phrases, and so on are specified for all languages or for certain language
classes. In comparison computational LFG grammars differ enormously in their c"=structures and
morphology is usually taken care of by external programs (Finite State Morphology) and is not part
of theoretical considerations.  Since for instance complex predicate formation in languages like
German and Persian interacts with derivational morphology, we consider it crucial that morphology is
dealt within the grammatical framework and that the computational implementation reflects the tight
connection between the two parts of grammar.


\section{Measuring Progress}
\label{sec-evaluation}

Much to the frustration of many linguists the contribution of certain approaches to progress in
linguistics is rather unclear. Many proposals do not extend the amount of data that is covered in
comparison to analyses that were developed during the 80ies in the framework of GB and other,
non-transformational frameworks. In comparison the methodology described in
Section~\ref{sec-pos-motivation} leads to grammars with increasing coverage and analyses that are
improved by cross-linguistic considerations.

The TRALE system has been combined with \tsdb, a software for systematic grammar profiling
\citep{OF98}. The grammars are accompanied with a set of example phrases that can be analyzed by the
grammar. In addition the test suite files contain ungrammatical word sequences from the literature
and ungrammatical sequences that were discovered during the grammar development process. See
\citew{ONK97a,Mueller2004f} on the construction of test suites. Since TRALE
has a chart display that makes it possible to inspect the parse chart, it is possible to inspect all
linguistic objects that are licensed by the grammar, even if they do not play a role in analyzing the particlular
sentence under consideration. The result of this careful inspection is a collection of ungrammatical
word sequences that no theoretical linguist would have been able to come up with since it is very
difficult to find all the side effects that an analysis might have that is not constrained
sufficiently. These negative examples are distributed with the grammars and book publications and
can help theoretical and computational linguists improve their theories and implementations.

After changing a grammar the sentences of the respective test suite are parsed and the result can be
compared to previous results. This way it is ensured that the coverage of grammars is extended. If
constraints in files are changed that are shared with other grammars the respective grammars are
tested as well. The effects that changes to grammar X cause in grammar Y are often unexpected and
hence it is very important to do systematic testing.
  

%\section{Challenges}



\section{Conclusions}
\label{sec-conclusions}

We argued that linguistic theories have reached a level of complexity that cannot be handled by
humans without help by computers. We discussed a method for constructing surface-oriented theories
by extending the number of languages that are considered and finaly provided a brief description of
basic assumptions and the basic setup of the CoreGram project.

\bibliography{bib-abbr,biblio,crossrefs}
\bibliographystyle{natbib.fullname}



%      <!-- Local IspellDict: 	en_US-w_accents -->

 
                              
\end{document}
      
